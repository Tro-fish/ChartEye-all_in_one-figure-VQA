<div align="center">

# ChartEye: Multi Modal QA Chatbot for Scientific Figure Images
<br><b> ChartEye: Vision for Your Scientific Chart & Figure </b> <br><br>
!!!!!!!!!ì—¬ê¸°ì— gif íŒŒì¼ ë„£ê¸°!!!!!!!!! <br> <br>
<p align="center">
<img src="https://github.com/user-attachments/assets/69ba6f4b-9691-46e4-9ef6-cf19cf4e9212" alt="Description of the image" width="100%" />
</p>

This repository contains the code for ChartEye, including its implementation and execution instructions, 
for the Figure-to-Caption and QA Chatbot for Chemistry and Materials Science documents.<br><br>
<b>ğŸ… Achievement: 2024 [Corning](https://www.corning.com/) AI Challenge Grand Prize ğŸ…</b>
</div>

## Main features
<b>ChartEye's main features include the following</b>
- Extract Chart(figure) image from PDF, PPTX, WORD files --> *Image Extraction*
- Convert Chart(figure) image to text explanation --> *Chart Captioning*
- Convert imformation in a Chart(figure) image to a number --> *Chart Derendering*
- Answer Question about the information in a Chart(figure) imge --> *Question Answering*


## Overall Pipeline of Chart Eye

<p align="center">
<img src="https://github.com/user-attachments/assets/5adbf683-9807-4e38-bf09-62dbfc9eea8a" alt="Description of the image" width="100%" />
</p>
<b>Step-by-step usage scenarios</b>  <br><br>
1. User uploads PDF, PPT, WORD files<br>
2. Extract images from the input files<br>
3. Extract only Figure images from the images<br>
4. User selects the figure image they want to analyze and answer questions with the chatbot<br>
6. Extract number, legend from selected figure image with OCR + create a caption<br>
7. User enters a question --> Input Question, Table, Image, and Caption into LLaVA model and generate answer<br>

## Install
### Setup `python` environment
```
conda create -n charteye python==3.10.10
```
### Install other dependencies
```
pip install -r requirements.txt
```
## Run Server
```
cd frontend
conda activate charteye
npm run dev --> ì´ê±° í•˜ê³  ì–´ë””ë¡œ ë“¤ì–´ê°€ì•¼í•´? localhost? ê·¸ê²ƒë„ ì ì–´ì¤˜.
```

## (ì°¸ê³ ) Repository Structure (ì°¸ê³ ) --> ìš°ë¦¬ë„ ì´ë ‡ê²Œ í•´ì•¼í•¨
The repository has two branches:
- `main` branch contains the code for customizing HuggingFace's implmentation of the UDOP model
- `custom` branch contains the code for customizing Microsoft's implmentation of the UDOP model


``` bash
.
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ config/                          # Train/Inference configuration files
â”‚   â”œâ”€â”€ inference.yaml
â”‚   â”œâ”€â”€ predict.yaml
â”‚   â””â”€â”€ train.yaml
â”œâ”€â”€ core/                            # Main UDOP/DataClass source code
â”‚   â”œâ”€â”€ common/
â”‚   â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ models/
â”‚   â””â”€â”€ trainers/
â”œâ”€â”€ data/                            # Custom dataset folder
â”‚   â”œâ”€â”€ images/
â”‚   â”‚   â””â”€â”€ image_{idx}.png
â”‚   â””â”€â”€ json_data/
â”‚       â””â”€â”€ processed_{idx}.pickle
â”œâ”€â”€ main.py                         # Main script to run training/inference
â”œâ”€â”€ models                          # Trained models saved to this folder
â”œâ”€â”€ sweep.py                        # Script to run hyperparameter sweep
â”œâ”€â”€ test                            # Save visualizations during inference
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ udop-unimodel-large-224         # Pretrained UDOP 224 model
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”œâ”€â”€ special_tokens_map.json
â”‚   â”œâ”€â”€ spiece.model
â”‚   â””â”€â”€ tokenizer_config.json
â”œâ”€â”€ udop-unimodel-large-512         # Pretrained UDOP 512 model
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”œâ”€â”€ special_tokens_map.json
â”‚   â”œâ”€â”€ spiece.model
â”‚   â””â”€â”€ tokenizer_config.json
â””â”€â”€ utils                           # Utilities
```

# Implementation Details (technical report)
In this section, we go into detail about how we implemented the functionality for each step.<br> If you simply want to use our application, you don't need to read it.
## Step1 - Image Extraction from files
<p align="center">
<img src="https://github.com/user-attachments/assets/a769a2df-9e95-4930-af72-39ee30d5b572" alt="Description of the image" width="100%" />
STEP1 Scenario: User uploads PDF, PPT, WORD files & Extract images from the input files</b><br>
</p>

- Extract images using external tools
    - PDF: Extracting images with [pymupdf](https://pymupdf.readthedocs.io/en/latest/)
    - WORD: Extracting images with [python-docx](https://python-docx.readthedocs.io/en/latest/)
    - PPTX: Extracting images with [python-pptx](https://python-pptx.readthedocs.io/en/latest/)
 
## Step2 - Figure Classification from image
<p align="center">
<img src="https://github.com/user-attachments/assets/632b3fff-a7d8-4cd1-b421-8ea968b82207" alt="Description of the image" width="100%" />
<img src="https://github.com/user-attachments/assets/e16b0251-d9cd-4cb6-9cc8-73c086ac22e8" alt="Description of the image" width="100%" />
<b>STEP2 Scenario: Extract only Figure images from the images & User selects the figure image they want to analyze and answer questions with the chatbot</b><br>
</p>

- Figure classficiation ëª¨ë¸ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ì„œ baseline ëª¨ë¸ë¡œ EfficientNet-B4ë¥¼ ì‚¬ìš©
- Pretrained EfficientNet-B4 ëª¨ë¸ì—ëŠ” figure, tableì— ëŒ€í•œ classê°€ ì—†ê¸° ë•Œë¬¸ì— ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•˜ê³  ì¶”ê°€ í•™ìŠµ ì§„í–‰
- ìš°ë¦¬ì˜ Figure classficiation ëª¨ë¸ì€ ì—¬ê¸°ì„œ í™•ì¸ ê°€ëŠ¥

  
<p align="center">
<img src="https://github.com/user-attachments/assets/36631d3d-1f5c-4da1-b98d-7dd594d1d266" alt="Description of the image" width="100%" />
<b>STEP2 Figure Classification dataset<br>
</p>



